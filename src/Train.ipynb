{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7_Fh2C_z93aV"
      },
      "outputs": [],
      "source": [
        "# check if it's a Colab notebook\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nxsi9ClOggVK",
        "outputId": "055dd4c3-0fab-48e7-cc60-44b89d39d067"
      },
      "outputs": [],
      "source": [
        "git_branch = 'origina_pipeline'\n",
        "\n",
        "\n",
        "import os\n",
        "if IN_COLAB:\n",
        "    from google.colab import runtime, userdata\n",
        "    !wget -q https://raw.githubusercontent.com/tsunrise/colab-github/main/colab_github.py\n",
        "    import colab_github\n",
        "    colab_github.github_auth(persistent_key=True)\n",
        "    # https://github.com/tsunrise/colab-github/tree/main\n",
        "\n",
        "    ! rm -rf submovement_detector\n",
        "    ! git clone \"REMOVED_FOR_ANONYMITY\"\n",
        "    %cd submovement_detector\n",
        "    !git checkout {git_branch}\n",
        "    # %cd ..\n",
        "\n",
        "    import sys\n",
        "    sys.path.append('/content/submovement_detector')\n",
        "\n",
        "    !pip install torchviz\n",
        "    !pip install fastkde\n",
        "\n",
        "    root_dir = f'/content/drive/MyDrive/submov_nn'\n",
        "    wandb_key = userdata.get('WANDB_KEY')\n",
        "    code_dir = '/content/submovement_detector'\n",
        "else:\n",
        "    root_dir = f'./submov_nn'\n",
        "    wandb_key = os.getenv('WANDB_KEY')\n",
        "    code_dir = '.'\n",
        "\n",
        "CONFIG_PATH = f'{root_dir}/config/config-0426-ModGaussian_shorter_kernel.yaml'\n",
        "USE_WANDB = True\n",
        "VIZUALIZE_BAD_AMPLITUDES = False\n",
        "\n",
        "datasets_dir = f'{root_dir}/data/'\n",
        "dataset2path = {\n",
        "     # 'crank1d':\n",
        "    'steering': os.path.join(datasets_dir, 'steering_tangential_velocity_data.csv'),\n",
        "    'crank': os.path.join(datasets_dir, 'crank_tangential_velocity_data.csv'),\n",
        "    'Fitts': os.path.join(datasets_dir, 'Fitts_tangential_velocity_data.csv'),\n",
        "    'whacamole': os.path.join(datasets_dir, 'whacamole_tangential_velocity_data.csv'),\n",
        "    'object_moving': os.path.join(datasets_dir, 'object_moving_tangential_velocity_data.csv'),\n",
        "    'pointing': os.path.join(datasets_dir, 'pointing_tangential_velocity_data.csv'),\n",
        "    'tablet_writing': os.path.join(datasets_dir, 'tablet_writing_tangential_velocity_data.csv'),\n",
        "}\n",
        "\n",
        "dataset_names = dataset2path.keys()\n",
        "# train_noise_condition = (10, 50)\n",
        "\n",
        "noise_conditions = [float('inf'), 20, 10]\n",
        "refractory_conditions = [(0., 0.5), (0.5, 1.), (1, 1.5)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "T1UCPg0Xi62H"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import copy\n",
        "from collections import defaultdict\n",
        "\n",
        "from itertools import product\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# from torch.utils.tensorboard import SummaryWriter  # TensorBoard writer\n",
        "\n",
        "import wandb\n",
        "\n",
        "from data import SyntheticDataset, OrganicDataset, CombinedSyntheticDataset\n",
        "from models import TDNNDetector, STEContinuousReconstructor, STEBinarizer\n",
        "from utils import onset_prediction_metrics_on_masks, Config, evaluate_on_organic_data, evaluate_on_synthetic_data\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def log(message, file):\n",
        "    with open(file, 'a') as f:\n",
        "        f.write(message + '\\n')\n",
        "    print(message)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6kJzw4KDV8M2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meugenartemovich\u001b[0m (\u001b[33mNAME_REMOVED\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.19.10 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/NAME_REMOVED/projects/submovement_detector/wandb/run-20250426_001200-euer1bkh</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/NAME_REMOVED/submovement_detector/runs/euer1bkh' target=\"_blank\">0426-ModGaussian_abs_velo</a></strong> to <a href='https://wandb.ai/NAME_REMOVED/submovement_detector' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/NAME_REMOVED/submovement_detector' target=\"_blank\">https://wandb.ai/NAME_REMOVED/submovement_detector</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/NAME_REMOVED/submovement_detector/runs/euer1bkh' target=\"_blank\">https://wandb.ai/NAME_REMOVED/submovement_detector/runs/euer1bkh</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "config = Config(CONFIG_PATH, root_dir=root_dir)\n",
        "\n",
        "# os.environ['WANDB_NOTEBOOK_NAME'] = 'Train.ipynb'\n",
        "\n",
        "\n",
        "if USE_WANDB:\n",
        "    wandb.login(key=wandb_key)\n",
        "    wandb.init(project='submovement_detector', name=config.experiment_name, config=config.to_dict() , save_code=True) #, settings=wandb.Settings(code_dir='.'))\n",
        "\n",
        "    wandb.run.log_code(code_dir, include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yinNJGiBNu-K"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "config-0426-ModGaussian_abs_velo\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/NAME_REMOVED/mambaforge/envs/article1/lib/python3.11/site-packages/torch/distributions/distribution.py:53: UserWarning: <class 'data.ModulatedGaussian'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# %%\n",
        "# Instantiate the config\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # Model\n",
        "    # dilation of 2 for most layers, 1 for the first and last\n",
        "    # additional output vector for masking (is not trained in the beginning)\n",
        "    # split train datasets into train and test\n",
        "    # from the test dataset, select only a fraction of it for testing\n",
        "\n",
        "    # Steps:\n",
        "    # 1. Start training from synthetic data (~5 epochs)\n",
        "    #   length (5 to 60 steps)\n",
        "    #   amplitude (-1 to 1) multiplied by duration\n",
        "    #   add jitter noise (10 to 50?)\n",
        "    #   refractory (0.1 to 1.5, clip to 3 steps minimum)\n",
        "    #   standardizing the trial afterwards to have std of 1\n",
        "    #   no reconstruction loss,\n",
        "    #   what about dropout and batchnorm?? should it starting without it\n",
        "    #   measuring performance on the train and test datasets the whole time\n",
        "    # 2. Train on organic data (from epoch ~5 onwards)\n",
        "    #   without reconstruction loss\n",
        "    #   generate 3 different classes of data from 3 different domains\n",
        "\n",
        "    # 3. Add reconstruction loss\n",
        "\n",
        "    # 4. Remove dropout and batchnorm?\n",
        "\n",
        "    # refractory_distributions = [(0.1, 1.5)]\n",
        "    # noise_conditions = [(10, 50)]\n",
        "    # conditions = [{'refractory_distribution': rd, 'snr_distribution': nc, 'experiment_name': f'Overlapping-{rd[0]}_{rd[1]}_Noise-{nc}-no_different_amplitude'}\n",
        "    #                 for nc, rd in product(noise_conditions, refractory_distributions)]\n",
        "\n",
        "    config = Config(CONFIG_PATH, root_dir=root_dir)\n",
        "    config.experiment_name = CONFIG_PATH.split('/')[-1].replace('.yaml', '')\n",
        "\n",
        "    #conditions = [{'refractory_distribution': config.refractory_distribution, 'snr_distribution': config.snr_distribution, 'experiment_name': CONFIG_PATH.split('/')[-1].replace('.yaml', '')}]\n",
        "\n",
        "    # conditions =\\\n",
        "    #     [{'refractory_distribution': (0.2, 1.5), 'snr_distribution': (10,50), 'experiment_name': 'Overlapping-0.2_1.5_Noise-10-40Fixed_Reconstruction5'}]\\\n",
        "    #         + conditions\n",
        "\n",
        "    # TensorBoard writer setup\n",
        "\n",
        "    # for condition in conditions:\n",
        "    # if condition['refractory_distribution'] == (1, 2) and condition['snr_distribution'] != 10:\n",
        "    #     continue\n",
        "\n",
        "    # if (condition['snr_distribution'] != 10) and (condition['refractory_distribution'] != (1, 2) or condition['snr_distribution'] != 20):\n",
        "    #     continue\n",
        "\n",
        "\n",
        "\n",
        "    # for key, value in condition.items():\n",
        "    #     setattr(config, key, value)\n",
        "\n",
        "    os.makedirs(os.path.dirname(config.log_file), exist_ok=True)\n",
        "    # writer = SummaryWriter(log_dir=config.log_dir)\n",
        "    print(config.experiment_name)\n",
        "\n",
        "    # Load the model\n",
        "    model = TDNNDetector(\n",
        "        batchnorm=config.batchnorm,\n",
        "        dilations=config.dilations,\n",
        "        channels=config.channels,\n",
        "        kernel_sizes=config.kernel_sizes,\n",
        "        num_layers=config.num_layers,\n",
        "        dropout_rate=config.dropout_rate,\n",
        "    ).to(config.device, config.dtype)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    latest_epoch = -1\n",
        "    if config.start_with_weights and config.start_with_weights != 'Xavier':\n",
        "        weights_file = None\n",
        "\n",
        "        if isinstance(config.start_with_weights, str):\n",
        "            weights_file = config.start_with_weights\n",
        "            weights_dir = '/'.join(config.weights_file.split('/')[:-1])\n",
        "            weights_file = os.path.join(weights_dir, weights_file)\n",
        "        else:\n",
        "            # find the latest weights file\n",
        "            weights_files = [f for f in os.listdir('/'.join(config.weights_file.split('/')[:-1])) if f.startswith(config.weights_file.split('/')[-1].replace('.pth', ''))]\n",
        "            if len(weights_files) > 0:\n",
        "                if isinstance(config.start_with_weights, bool):\n",
        "                    epoch_numbers = [int(f.split('_')[-1].replace('.pth', '')) for f in weights_files]\n",
        "                    latest_epoch = max(epoch_numbers)\n",
        "                elif isinstance(config.start_with_weights, int):\n",
        "                    latest_epoch = config.start_with_weights\n",
        "                else:\n",
        "                    raise ValueError(f'Invalid start_with_weights value: {config.start_with_weights}')\n",
        "                weights_file = config.weights_file.replace('.pth', f'_{latest_epoch}.pth')\n",
        "        if weights_file is not None:\n",
        "            model.load_state_dict(torch.load(weights_file, map_location=config.device))\n",
        "            print(f'Loaded weights from {weights_file}, continuing from epoch {latest_epoch}')\n",
        "        else:\n",
        "            print(f'No weights file found, starting from scratch')\n",
        "\n",
        "    # Define the loss functions and optimizer\n",
        "    criterion_entropy = nn.BCELoss()\n",
        "    criterion_entropy_wo_reduction = nn.BCELoss(reduction='none')\n",
        "    criterion_mse = nn.MSELoss()\n",
        "\n",
        "    basic_dataset = SyntheticDataset(**config.get_dataset_parameters())\n",
        "\n",
        "    reconstructor = basic_dataset.reconstruction_model\n",
        "\n",
        "    if 'stat_snr_distribution':\n",
        "        train_noise_condition = config.stat_snr_distribution\n",
        "    else:\n",
        "        train_noise_condition = (10, 50)\n",
        "    dataset2stats_path = {\n",
        "        dataset_name: os.path.join(datasets_dir, f'{config.experiment_name}-{dataset_name}-{train_noise_condition}-train-pulled_stats.csv')\n",
        "        for dataset_name in dataset_names\n",
        "    }\n",
        "\n",
        "    noise_conditions_train = [train_noise_condition]\n",
        "    if config.combined_dataset:\n",
        "\n",
        "        dataset2path_train = {k: v for k, v in dataset2path.items() if k in config.datasets}\n",
        "\n",
        "        evaluate_on_organic_data(\n",
        "            model=model,\n",
        "            dataset2path=dataset2path_train,\n",
        "            noise_conditions=noise_conditions_train,\n",
        "            config=config,\n",
        "            reconstructor=reconstructor,\n",
        "            step=0,\n",
        "            purpose='train',\n",
        "            low_pass_filter=np.inf,\n",
        "            save_pulled_stats=config.datasets_dir\n",
        "        )\n",
        "\n",
        "        stats_datasets = []\n",
        "        for dataset_name in config.datasets:\n",
        "            file_path = dataset2stats_path[dataset_name]\n",
        "            dataset = SyntheticDataset(joint_distribution=file_path, **config.get_dataset_parameters())\n",
        "            stats_datasets.append(dataset)\n",
        "\n",
        "        dataset = CombinedSyntheticDataset(stats_datasets + [basic_dataset], proportions=config.proportions,\n",
        "                                                    total_duration_distribution=config.total_duration_distribution,\n",
        "                                                    batch_size=config.batch_size, dtype=config.dtype, device=config.device)\n",
        "    else:\n",
        "        dataset = basic_dataset\n",
        "\n",
        "    # Load the dataset and dataloader\n",
        "    # dataset = SyntheticDataset(**config.get_dataset_parameters())\n",
        "    dataloader = data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "\n",
        "    # lr exponential decay\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "\n",
        "    scheduler_start = config.lr_decay_start\n",
        "    scheduler_end = config.lr_decay_end\n",
        "    scheduler_total_decay = config.lr_decay_total_change\n",
        "    step_decay = scheduler_total_decay ** (1/((scheduler_end - scheduler_start) * len(dataloader)))\n",
        "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=step_decay)\n",
        "\n",
        "    # Reconstruction model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "I5dZJtz0Ae-D"
      },
      "outputs": [],
      "source": [
        "if 'start_test' in config.to_dict() and config.start_test:\n",
        "    model.eval()\n",
        "\n",
        "    evaluate_on_synthetic_data(\n",
        "                    model=model,\n",
        "                    noise_conditions=noise_conditions,\n",
        "                    refractory_conditions=refractory_conditions,\n",
        "                    config=config,\n",
        "                    reconsturctor=reconstructor,\n",
        "            step=0\n",
        "        )\n",
        "    evaluate_on_organic_data(\n",
        "                    model=model,\n",
        "                    dataset2path=dataset2path,\n",
        "                    noise_conditions=noise_conditions,\n",
        "                    config=config,\n",
        "                    reconstructor=reconstructor,\n",
        "                    step=0\n",
        "                )\n",
        "    model.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jh-xhy8pDeeB"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    # intialize with Xavier\n",
        "    if config.start_with_weights == 'Xavier':\n",
        "        for param in model.parameters():\n",
        "            if isinstance(param, nn.Conv1d) or isinstance(param, nn.Linear):\n",
        "                nn.init.xavier_normal_(param)\n",
        "    tm = time.time()\n",
        "\n",
        "\n",
        "    for epoch in range(latest_epoch+1, config.num_epochs):\n",
        "        dataset.seed = epoch  # Update seed for each epoch\n",
        "        detection_loss_mean = 0\n",
        "        duration_loss_mean = 0\n",
        "        amplitude_loss_mean = 0\n",
        "        reconstruction_loss_mean = 0\n",
        "        reconstruction_detection_loss_mean = 0\n",
        "        mean_onset_precision = 0\n",
        "        mean_onset_recall = 0\n",
        "        mean_onset_distance = 0\n",
        "\n",
        "        if epoch >= config.reconstruction_loss_start:\n",
        "            config.use_reconstruction_loss = True\n",
        "\n",
        "        if epoch >= config.bn_dropout_freeze_start:\n",
        "            if config.batchnorm:\n",
        "                for batch_norm in model.batchnorm_layers:\n",
        "                    batch_norm.eval()\n",
        "            if config.dropout_rate > 0:\n",
        "                model.dropout.eval()\n",
        "\n",
        "        for i, data_ in enumerate(dataloader):\n",
        "            if epoch >= scheduler_start and epoch < scheduler_end:\n",
        "                scheduler.step()\n",
        "            x, x_clean, y = data_\n",
        "            if x.device != config.device or x.dtype != config.dtype:\n",
        "                x = x.to(config.device, config.dtype)\n",
        "                y = y.to(config.device, config.dtype)\n",
        "            x = x.squeeze(0)\n",
        "            x_clean = x_clean.squeeze(0)\n",
        "            y = y.squeeze(0)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(x)\n",
        "\n",
        "            mask = y[:, 0]\n",
        "            amplitude = y[:, 1]\n",
        "            duration = y[:, 2]\n",
        "\n",
        "            mask_pred = y_pred[:, 0]\n",
        "            amplitude_pred = y_pred[:, 1]\n",
        "            duration_pred = y_pred[:, 2]\n",
        "            if y_pred.shape[1] == 4:\n",
        "                reconstruction_mask_pred = y_pred[:, 3]\n",
        "            else:\n",
        "                reconstruction_mask_pred = None\n",
        "\n",
        "            # Compute losses\n",
        "            if hasattr(config, 'weight_with_amplitude') and config.weight_with_amplitude:\n",
        "                detection_loss_positives = criterion_entropy_wo_reduction(mask_pred[mask == 1], mask[mask == 1])\n",
        "                abs_amplitudes = torch.abs(amplitude[mask == 1])\n",
        "                abs_amplitudes = torch.sqrt(abs_amplitudes)\n",
        "                abs_amplitudes = abs_amplitudes.detach()\n",
        "                detection_loss_positives *= abs_amplitudes\n",
        "                detection_loss_positives = detection_loss_positives.mean() / abs_amplitudes.mean()             \n",
        "            else:\n",
        "                detection_loss_positives = criterion_entropy(mask_pred[mask == 1], mask[mask == 1])\n",
        "\n",
        "            if hasattr(config, 'weight_up_to_n_neighbors') and config.weight_up_to_n_neighbors > 0:\n",
        "                weight_up_to_n_neighbors = config.weight_up_to_n_neighbors\n",
        "                weighting_factor = 0.5\n",
        "                # detection_loss_negatives = 0\n",
        "                scaling_exponent_tensor = torch.zeros_like(mask)\n",
        "                for j in range(1, weight_up_to_n_neighbors + 1):\n",
        "                    scaling_exponent_tensor[:, j:] = torch.max(scaling_exponent_tensor[:, j:], mask[:, :-j] * (weight_up_to_n_neighbors - j + 1))\n",
        "                    scaling_exponent_tensor[:, :-j] = torch.max(scaling_exponent_tensor[:, :-j], mask[:, j:] * (weight_up_to_n_neighbors - j + 1))\n",
        "                weight_tensor = torch.pow(weighting_factor, scaling_exponent_tensor)\n",
        "                weight_tensor[mask == 1] = 0\n",
        "                weight_tensor = weight_tensor.detach()\n",
        "                detection_loss_negatives = criterion_entropy_wo_reduction(mask_pred[mask == 0], mask[mask == 0]) * weight_tensor[mask == 0]\n",
        "                detection_loss_negatives = detection_loss_negatives.mean() / weight_tensor[mask == 0].mean()\n",
        "            else:\n",
        "                detection_loss_negatives = criterion_entropy(mask_pred[mask == 0], mask[mask == 0])\n",
        "\n",
        "            if hasattr(config, 'negative_loss_multiplier'):\n",
        "                if config.negative_loss_multiplier == 'adaptive' or config.negative_loss_multiplier == 'balanced':\n",
        "                    binarized_mask_pred = reconstructor.binarizer.apply(mask_pred, False, True)\n",
        "                    binarized_mask_pred = binarized_mask_pred.detach()\n",
        "                    pred_to_true_ratio = binarized_mask_pred.sum() / mask.sum()\n",
        "                    if config.negative_loss_multiplier == 'balanced' and pred_to_true_ratio < 1.0:\n",
        "                        detection_loss_positives /= pred_to_true_ratio ** 0.5\n",
        "                        # when there are not enough positives, we punish for false negatives more\n",
        "                        # but not as much as for false positives\n",
        "                    pred_to_true_ratio = torch.clamp(pred_to_true_ratio, 1.0, 10.0)\n",
        "                    detection_loss_negatives *= pred_to_true_ratio\n",
        "                elif isinstance(config.negative_loss_multiplier, (int, float)):\n",
        "                    detection_loss_negatives *= config.negative_loss_multiplier\n",
        "            detection_loss = detection_loss_positives + detection_loss_negatives\n",
        "\n",
        "            original_duration_loss = criterion_mse(duration_pred[mask == 1], duration[mask == 1])\n",
        "            original_amplitude_loss = criterion_mse(amplitude_pred[mask == 1], amplitude[mask == 1])\n",
        "\n",
        "            if VIZUALIZE_BAD_AMPLITUDES and original_amplitude_loss.item() > 1000:\n",
        "                print('='*100)\n",
        "                print(f'Epoch: {epoch}, Iteration: {i}')\n",
        "                print(f'Amplitude loss is too high: {original_amplitude_loss.item()}')\n",
        "                print(f'Amplitude true: {amplitude[mask == 1]}')\n",
        "                print(f'Amplitude pred: {amplitude_pred[mask == 1]}')\n",
        "                for bt_el in range(amplitude.shape[0]):\n",
        "                    onset_indices = np.where(mask[bt_el].cpu().numpy() == 1)[0]\n",
        "                    bt_el_with_anomaly = False\n",
        "                    for onset_index in onset_indices:\n",
        "                        mse = (amplitude[bt_el, onset_index] - amplitude_pred[bt_el, onset_index])**2\n",
        "                        if mse > 5000:\n",
        "                            bt_el_with_anomaly = True\n",
        "                            print(f'MSE: {mse}, Amplitude true: {amplitude[bt_el, onset_index]}, Amplitude pred: {amplitude_pred[bt_el, onset_index]}')\n",
        "                    if bt_el_with_anomaly:\n",
        "                        print(f'BT element {bt_el} has anomalies')\n",
        "                        plt.figure(figsize=(10, 5), dpi=200)\n",
        "                        plt.plot(amplitude[bt_el].detach().cpu().numpy())\n",
        "                        plt.title('Amplitude')\n",
        "                        plt.show()\n",
        "                        plt.figure(figsize=(10, 5), dpi=200)\n",
        "                        plt.plot(amplitude_pred[bt_el].detach().cpu().numpy())\n",
        "                        plt.title('Amplitude pred')\n",
        "                        plt.show()\n",
        "                        plt.figure(figsize=(10, 5), dpi=200)\n",
        "                        plt.plot(x[bt_el][0].detach().cpu().numpy())\n",
        "                        plt.title('Signal')\n",
        "                        plt.show()\n",
        "                    print(f'x mean: {x[bt_el].mean().item()}, x ala std {np.sqrt((x[bt_el]**2).cpu().numpy()).mean().item()}')\n",
        "                    print(f'amplitude mean: {amplitude[bt_el].mean().item()}')\n",
        "                    print(f'amplitude pred mean: {amplitude_pred[bt_el].mean().item()}')\n",
        "\n",
        "                print('='*100)\n",
        "\n",
        "            if y_pred.shape[1] == 4:\n",
        "                reconstruction_detection_loss_positives = criterion_entropy(reconstruction_mask_pred[mask == 1], mask[mask == 1])\n",
        "                reconstruction_detection_loss_negatives = criterion_entropy(reconstruction_mask_pred[mask == 0], mask[mask == 0])\n",
        "                original_reconstruction_detection_loss = reconstruction_detection_loss_positives + reconstruction_detection_loss_negatives\n",
        "\n",
        "                reconstruction_detection_loss_mean *= i / (i + 1)\n",
        "                reconstruction_detection_loss_mean += original_reconstruction_detection_loss.item() / (i + 1)\n",
        "                reconstruction_detection_loss = original_reconstruction_detection_loss / reconstruction_detection_loss_mean * detection_loss_mean\n",
        "                reconstruction_detection_loss *= 0.1 # decrease the weight of the reconstruction detection loss\n",
        "\n",
        "                # writer.add_scalar('Loss/ReconstructionDetection', original_reconstruction_detection_loss.item(), epoch * len(dataloader) + i)\n",
        "                if USE_WANDB:\n",
        "                    wandb.log({'Loss/ReconstructionDetection': original_reconstruction_detection_loss.item()}, step=epoch * len(dataloader) + i)\n",
        "\n",
        "\n",
        "            # Update running means\n",
        "            detection_loss_mean *= i / (i + 1)\n",
        "            detection_loss_mean += detection_loss.item() / (i + 1)\n",
        "            duration_loss_mean *= i / (i + 1)\n",
        "            duration_loss_mean += original_duration_loss.item() / (i + 1)\n",
        "            amplitude_loss_mean *= i / (i + 1)\n",
        "            amplitude_loss_mean += original_amplitude_loss.item() / (i + 1)\n",
        "\n",
        "            # Normalize the losses\n",
        "            duration_loss = original_duration_loss / duration_loss_mean * detection_loss_mean\n",
        "            amplitude_loss = original_amplitude_loss / amplitude_loss_mean * detection_loss_mean\n",
        "\n",
        "            # Compute reconstruction loss if applicable\n",
        "            reconstructed_x, _ = reconstructor(y_pred)\n",
        "            if not config.use_reconstruction_loss:\n",
        "                reconstructed_x = reconstructed_x.detach()\n",
        "            original_reconstruction_loss = criterion_mse(reconstructed_x, x_clean)\n",
        "            reconstruction_loss_mean *= i / (i + 1)\n",
        "            reconstruction_loss_mean += original_reconstruction_loss.item() / (i + 1)\n",
        "            reconstruction_loss = original_reconstruction_loss / reconstruction_loss_mean * detection_loss_mean\n",
        "\n",
        "            # Total loss\n",
        "            loss = detection_loss + duration_loss + amplitude_loss\n",
        "            if config.use_reconstruction_loss:\n",
        "                loss += reconstruction_loss\n",
        "            elif y_pred.shape[1] == 4:\n",
        "                loss += reconstruction_detection_loss\n",
        "\n",
        "            # Backpropagation and optimizer step\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if USE_WANDB:\n",
        "                wandb.log({'Loss/Total': loss.item(),\n",
        "                        'Loss/Detection': detection_loss.item(),\n",
        "                        'Loss/Duration': original_duration_loss.item(),\n",
        "                        'Loss/Amplitude': original_amplitude_loss.item(),\n",
        "                        'Loss/Reconstruction': original_reconstruction_loss.item()}, step=epoch * len(dataloader) + i)\n",
        "\n",
        "            # Onset metrics and printing progress\n",
        "            if i % config.log_interval == 0:\n",
        "                _, _, _, precision, recall, _, distance = onset_prediction_metrics_on_masks(mask, mask_pred)\n",
        "                mean_onset_precision *= (i // config.log_interval) / (i // config.log_interval + 1)\n",
        "                mean_onset_precision += precision / (i // config.log_interval + 1)\n",
        "                mean_onset_recall *= (i // config.log_interval) / (i // config.log_interval + 1)\n",
        "                mean_onset_recall += recall / (i // config.log_interval + 1)\n",
        "                mean_onset_distance *= (i // config.log_interval) / (i // config.log_interval + 1)\n",
        "                mean_onset_distance += distance / (i // config.log_interval + 1)\n",
        "\n",
        "                if USE_WANDB:\n",
        "                    wandb.log({'Onset/Precision': precision,\n",
        "                            'Onset/Recall': recall,\n",
        "                            'Onset/Distance': distance}, step=epoch * len(dataloader) + i)\n",
        "\n",
        "                if USE_WANDB:\n",
        "                    wandb.log({'Params/LearningRate': scheduler.get_last_lr()[0]}, step=epoch * len(dataloader) + i)\n",
        "\n",
        "                message =   f'Epoch {epoch}, Iteration {i}, Loss: {loss.item()},'\\\n",
        "                            f'\\nDetection Loss: {detection_loss.item()},'\\\n",
        "                            f'\\nDuration Loss: {original_duration_loss.item()}, Amplitude Loss: {original_amplitude_loss.item()},'\\\n",
        "                            f'\\nReconstruction Loss: {original_reconstruction_loss.item()}'\\\n",
        "                            f'\\nOnset Precision: {precision}, Onset Recall: {recall}, Onset Distance: {distance}'\\\n",
        "                            f'\\nTime: {time.time() - tm}'\n",
        "                log(message, config.log_file)\n",
        "\n",
        "\n",
        "            # Plot reconstructions at intervals\n",
        "            if i % config.plot_interval == 0:\n",
        "                plt.figure(figsize=(10, 5), dpi=200)\n",
        "                ids_to_plot = random.sample(range(len(x)), config.reconstructions_to_plot)\n",
        "                for j, id_ in enumerate(ids_to_plot):\n",
        "\n",
        "                    num_rows = 2\n",
        "                    num_cols = math.ceil(config.reconstructions_to_plot / 2)\n",
        "                    plt.subplot(num_rows, num_cols, j + 1)\n",
        "                    original_signal = x[id_].squeeze().detach().cpu().numpy()\n",
        "                    clean_signal = x_clean[id_].squeeze().detach().cpu().numpy()\n",
        "                    reconstructed_signal = reconstructed_x[id_].squeeze().detach().cpu().numpy()\n",
        "                    ts = np.arange(len(original_signal)) / 60\n",
        "                    plt.plot(ts, original_signal)\n",
        "                    plt.plot(ts, reconstructed_signal, linestyle='--')\n",
        "                    plt.plot(ts, clean_signal, linestyle=':')\n",
        "                    # plt.title(f'Original vs Reconstructed Signal {j}')\n",
        "                    if j == 0:\n",
        "                        plt.legend(['Original', 'Reconstructed', 'Clean'])\n",
        "                    if j % num_cols == 0:\n",
        "                        plt.ylabel('Amplitude, a.u.')\n",
        "                    if j >= num_cols * (num_rows - 1):\n",
        "                        plt.xlabel('Time (s)')\n",
        "                    plt.grid()\n",
        "                # plt.tight_layout()\n",
        "                plt.suptitle(f'Original vs Reconstructed Signal Examples, Epoch {epoch}, Step {i}')\n",
        "\n",
        "                if USE_WANDB:\n",
        "                    wandb.log({'Reconstructions': wandb.Image(plt)}, step=epoch * len(dataloader) + i)\n",
        "\n",
        "                plt.show()\n",
        "\n",
        "            # break\n",
        "\n",
        "        # Log mean losses and onset metrics for the epoch\n",
        "        if USE_WANDB:\n",
        "            wandb.log({'Loss_Epoch/Detection_Mean': detection_loss_mean,\n",
        "                        'Loss_Epoch/Duration_Mean': duration_loss_mean,\n",
        "                        'Loss_Epoch/Amplitude_Mean': amplitude_loss_mean,\n",
        "                        'Loss_Epoch/Reconstruction_Mean': reconstruction_loss_mean,\n",
        "                        'Onset_Epoch/Precision_Mean': mean_onset_precision,\n",
        "                        'Onset_Epoch/Recall_Mean': mean_onset_recall,\n",
        "                        'Onset_Epoch/Distance_Mean': mean_onset_distance}, step=len(dataloader) * (epoch+1))\n",
        "\n",
        "        # Save model weights at the end of the epoch\n",
        "        os.makedirs(os.path.dirname(config.weights_file), exist_ok=True)\n",
        "        torch.save(model.state_dict(), config.weights_file.replace('.pth', f'_{epoch}.pth'))\n",
        "\n",
        "        message =   f'Epoch {epoch} finished,'\\\n",
        "                    f'\\nDetection Loss: {detection_loss_mean},'\\\n",
        "                    f'\\nDuration Loss: {duration_loss_mean}, Amplitude Loss: {amplitude_loss_mean},'\\\n",
        "                    f'\\nReconstruction Loss: {reconstruction_loss_mean}'\\\n",
        "                    f'\\nOnset Precision: {mean_onset_precision}, Onset Recall: {mean_onset_recall}, Onset Distance: {mean_onset_distance}'\\\n",
        "                    f'\\nTime: {time.time() - tm}'\n",
        "        log(message, config.log_file)\n",
        "\n",
        "        model.eval()\n",
        "        if (epoch+1) % 1 == 0:\n",
        "            evaluate_on_synthetic_data(\n",
        "                            model=model,\n",
        "                            noise_conditions=noise_conditions,\n",
        "                            refractory_conditions=refractory_conditions,\n",
        "                            config=config,\n",
        "                            reconsturctor=reconstructor,\n",
        "                    step=len(dataloader) * (epoch+1)\n",
        "                )\n",
        "        if (epoch+1) % 5 == 0:\n",
        "            evaluate_on_organic_data(\n",
        "                            model=model,\n",
        "                            dataset2path=dataset2path,\n",
        "                            noise_conditions=noise_conditions,\n",
        "                            config=config,\n",
        "                            reconstructor=reconstructor,\n",
        "                            step=len(dataloader) * (epoch+1)\n",
        "                        )\n",
        "\n",
        "        if (epoch+1) % 1 == 0:\n",
        "            if config.combined_dataset:\n",
        "                dataset2path_ = dataset2path_train\n",
        "            else:\n",
        "                dataset2path_ = {}\n",
        "\n",
        "\n",
        "            evaluate_on_organic_data(\n",
        "                model=model,\n",
        "                dataset2path=dataset2path_,\n",
        "                noise_conditions=noise_conditions_train, \n",
        "                config=config,\n",
        "                reconstructor=reconstructor,\n",
        "                step=len(dataloader) * (epoch+1),\n",
        "                purpose='train',\n",
        "                low_pass_filter=np.inf,\n",
        "                save_pulled_stats=config.datasets_dir\n",
        "            )\n",
        "            stats_datasets = []\n",
        "            if 'datasets' in config.to_dict():\n",
        "                for dataset_name in config.datasets:\n",
        "                    file_path = dataset2stats_path[dataset_name]\n",
        "                    dataset = SyntheticDataset(joint_distribution=file_path, **config.get_dataset_parameters())\n",
        "                    stats_datasets.append(dataset)\n",
        "\n",
        "                dataset = CombinedSyntheticDataset(stats_datasets + [basic_dataset], proportions=config.proportions,\n",
        "                                                            total_duration_distribution=config.total_duration_distribution,\n",
        "                                                            batch_size=config.batch_size, dtype=config.dtype, device=config.device)\n",
        "                \n",
        "                dataloader = data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "        model.train()\n",
        "        # save tensorboard logs\n",
        "        # writer.flush()\n",
        "\n",
        "\n",
        "    # Close the writer after training\n",
        "    #writer.close()\n",
        "\n",
        "    # %%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.1862, 0.0931, 0.0466,  ..., 0.7450, 0.7450, 0.7450],\n",
              "        [0.7450, 0.7450, 0.7450,  ..., 0.7450, 0.7450, 0.7450],\n",
              "        [0.7450, 0.7450, 0.7450,  ..., 0.7450, 0.7450, 0.7450],\n",
              "        ...,\n",
              "        [0.3725, 0.1862, 0.0931,  ..., 0.7450, 0.7450, 0.7450],\n",
              "        [0.7450, 0.7450, 0.7450,  ..., 0.7450, 0.7450, 0.7450],\n",
              "        [0.7450, 0.7450, 0.7450,  ..., 0.7450, 0.7450, 0.7450]],\n",
              "       grad_fn=<MulBackward0>)"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0SgFQX7Okhx"
      },
      "outputs": [],
      "source": [
        "if USE_WANDB:\n",
        "    wandb.finish()\n",
        "if IN_COLAB:\n",
        "    runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUawDb6uDeeC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "article1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
